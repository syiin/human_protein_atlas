{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "from fastai.dataset import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './'\n",
    "TRAIN = 'data/train/'\n",
    "TEST = 'data/test/'\n",
    "LABELS = 'data/train.csv'\n",
    "SAMPLE = 'data/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = list({f[:36] for f in os.listdir(TRAIN)})\n",
    "test_names = list({f[:36] for f in os.listdir(TEST)})\n",
    "tr_n, val_n = train_test_split(train_names, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_rgby(path,id): \n",
    "    colors = ['red','green','blue','yellow']\n",
    "    flags = cv2.IMREAD_GRAYSCALE\n",
    "    img = [cv2.imread(os.path.join(path, id+'_'+color+'.png'), flags).astype(np.float32)/255\n",
    "           for color in colors]\n",
    "    return np.stack(img, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_label_dict = {\n",
    "0:  'Nucleoplasm',\n",
    "1:  'Nuclear membrane',\n",
    "2:  'Nucleoli',   \n",
    "3:  'Nucleoli fibrillar center',\n",
    "4:  'Nuclear speckles',\n",
    "5:  'Nuclear bodies',\n",
    "6:  'Endoplasmic reticulum',   \n",
    "7:  'Golgi apparatus',\n",
    "8:  'Peroxisomes',\n",
    "9:  'Endosomes',\n",
    "10:  'Lysosomes',\n",
    "11:  'Intermediate filaments',\n",
    "12:  'Actin filaments',\n",
    "13:  'Focal adhesion sites',   \n",
    "14:  'Microtubules',\n",
    "15:  'Microtubule ends',  \n",
    "16:  'Cytokinetic bridge',   \n",
    "17:  'Mitotic spindle',\n",
    "18:  'Microtubule organizing center',  \n",
    "19:  'Centrosome',\n",
    "20:  'Lipid droplets',\n",
    "21:  'Plasma membrane',   \n",
    "22:  'Cell junctions', \n",
    "23:  'Mitochondria',\n",
    "24:  'Aggresome',\n",
    "25:  'Cytosol',\n",
    "26:  'Cytoplasmic bodies',   \n",
    "27:  'Rods & rings' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pdFilesDataset(FilesDataset):\n",
    "    def __init__(self, fnames, path, transform):\n",
    "        self.labels = pd.read_csv(LABELS).set_index('Id')\n",
    "        self.labels['Target'] = [[int(i) for i in s.split()] for s in self.labels['Target']]\n",
    "        super().__init__(fnames, transform, path)\n",
    "    \n",
    "    def get_x(self, i):\n",
    "        img = open_rgby(self.path,self.fnames[i])\n",
    "        if self.sz == 512: return img \n",
    "        else: return cv2.resize(img, (self.sz, self.sz),cv2.INTER_AREA)\n",
    "    \n",
    "    def get_y(self, i):\n",
    "        if(self.path == TEST): return np.zeros(len(name_label_dict),dtype=np.int)\n",
    "        else:\n",
    "            labels = self.labels.loc[self.fnames[i]]['Target']\n",
    "            return np.eye(len(name_label_dict),dtype=np.float)[labels].sum(axis=0)\n",
    "        \n",
    "    @property\n",
    "    def is_multi(self): return True\n",
    "    @property\n",
    "    def is_reg(self):return True\n",
    "    #this flag is set to remove the output sigmoid that allows log(sigmoid) optimization\n",
    "    #of the numerical stability of the loss function\n",
    "    \n",
    "    def get_c(self): return len(name_label_dict) #number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sz,bs):\n",
    "    #data augmentation\n",
    "    aug_tfms = [RandomRotate(30, tfm_y=TfmType.NO),\n",
    "                RandomDihedral(tfm_y=TfmType.NO)]\n",
    "    stats = A([0.00505, 0.00331, 0.00344, 0.00519], [0.10038, 0.08131, 0.08284, 0.10179])\n",
    "    tfms = tfms_from_stats(stats, sz, crop_type=CropType.NO, tfm_y=TfmType.NO, \n",
    "                aug_tfms=aug_tfms)\n",
    "    ds = ImageData.get_ds(pdFilesDataset, (tr_n[:-(len(tr_n)%bs)],TRAIN), \n",
    "                (val_n,TRAIN), tfms, test=(test_names,TEST))\n",
    "    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n",
    "    return md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "folds = 5\n",
    "train_folds = []\n",
    "val_folds = []\n",
    "\n",
    "def make_folds():\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=33, shuffle=True)\n",
    "    \n",
    "    train_names = list({f[:36] for f in os.listdir(TRAIN)})\n",
    "    test_names = list({f[:36] for f in os.listdir(TEST)})\n",
    "    trn_df = pd.read_csv('data/train.csv')\n",
    "    \n",
    "    for train_index, evaluate_index in skf.split(trn_df.index.values, trn_df.Target):\n",
    "        trn_value = trn_df.iloc[train_index]\n",
    "        val_value = trn_df.iloc[evaluate_index]\n",
    "        train_folds.append(trn_value)\n",
    "        val_folds.append(val_value)\n",
    "        print(train_index.shape, evaluate_index.shape)\n",
    "    \n",
    "    for i in range(folds):\n",
    "        train_folds[i].to_csv(f'data/5_fold/trn_folds_{i}')\n",
    "        val_folds[i].to_csv(f'data/5_fold/val_folds_{i}')\n",
    "\n",
    "def load_folds():\n",
    "    for i in range(folds):\n",
    "        train_folds.append(pd.read_csv(f'data/5_fold/trn_folds_{i}'))\n",
    "        val_folds.append(pd.read_csv(f'data/5_fold/val_folds_{i}'))\n",
    "        print(len(train_folds[i]), len(val_folds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24673 6399\n",
      "24836 6236\n",
      "24710 6362\n",
      "24997 6075\n",
      "25072 6000\n"
     ]
    }
   ],
   "source": [
    "# make_folds()\n",
    "load_folds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "folds = 5\n",
    "over_train_folds = []\n",
    "over_val_folds = []\n",
    "\n",
    "def make_over_folds():\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=33, shuffle=True)\n",
    "    \n",
    "    trn_df = pd.read_csv('data/train.csv')\n",
    "    \n",
    "    for train_index, evaluate_index in skf.split(trn_df.index.values, trn_df.Target):\n",
    "        trn_value = trn_df.iloc[train_index]\n",
    "        val_value = trn_df.iloc[evaluate_index]\n",
    "        \n",
    "        over_train_folds.append(trn_value)\n",
    "        over_val_folds.append(val_value)\n",
    "        print(train_index.shape, evaluate_index.shape)\n",
    "    \n",
    "    for k in range(folds):\n",
    "        trn_fold_frame = over_train_folds[k].copy()\n",
    "        val_fold_frame = over_val_folds[k].copy()\n",
    "        \n",
    "        trn_unique = trn_fold_frame[~trn_fold_frame['Target'].duplicated(keep=False)]\n",
    "        val_unique = val_fold_frame[~val_fold_frame['Target'].duplicated(keep=False)]\n",
    "        \n",
    "        trn_unique = pd.concat([trn_unique]*100, ignore_index=True)\n",
    "        val_unique = pd.concat([val_unique]*20, ignore_index=True)\n",
    "        \n",
    "        over_train_folds[k] = pd.concat([trn_fold_frame, trn_unique], ignore_index=True)\n",
    "        over_val_folds[k] = pd.concat([val_fold_frame, val_unique], ignore_index=True)\n",
    "        \n",
    "    for i in range(folds):\n",
    "        over_train_folds[i].to_csv(f'data/5_fold/over_sampled/trn_folds_{i}')\n",
    "        over_val_folds[i].to_csv(f'data/5_fold/over_sampled/val_folds_{i}')\n",
    "\n",
    "def load_over_folds():\n",
    "    for i in range(folds):\n",
    "        over_train_folds.append(pd.read_csv(f'data/5_fold/over_sampled/trn_folds_{i}'))\n",
    "        over_val_folds.append(pd.read_csv(f'data/5_fold/over_sampled/val_folds_{i}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24673,) (6399,)\n",
      "(24836,) (6236,)\n",
      "(24710,) (6362,)\n",
      "(24997,) (6075,)\n",
      "(25072,) (6000,)\n"
     ]
    }
   ],
   "source": [
    "# make_over_folds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_over_folds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(folds):\n",
    "    print(len(over_train_folds[i]))\n",
    "    print(len(over_val_folds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.sum(dim=1).mean()\n",
    "    \n",
    "def acc(preds,targs,thresh=0.0):\n",
    "    preds = (preds > thresh).int()\n",
    "    targs = targs.int()\n",
    "    return (preds==targs).float().mean()\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    epsilon = 1e-7\n",
    "    y_pred = torch.sigmoid(y_pred)\n",
    "    y_true = y_true.type(torch.FloatTensor).cuda()\n",
    "    tp = torch.sum(y_true*y_pred)\n",
    "    tn = torch.sum((1-y_true)*(1-y_pred))\n",
    "    fp = torch.sum((1-y_true)*y_pred)\n",
    "    fn = torch.sum((y_true*(1-y_pred)))\n",
    "    \n",
    "    p = tp / (tp + fp + epsilon)\n",
    "    r = tp / (tp + fn + epsilon)\n",
    "    f1 = 2*p*r / (p+r+epsilon)\n",
    "    f1 = torch.where(torch.isnan(f1), torch.zeros_like(f1), f1)\n",
    "    return 2 - torch.mean(f1)\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    y_pred = torch.sigmoid(y_pred)\n",
    "    y_true = y_true.type(torch.FloatTensor).cuda()\n",
    "    score = torch.sum(2.0*(y_pred*y_true))/torch.sum((y_pred+y_true) + 1e-7)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvnetBuilder_custom():\n",
    "    def __init__(self, f, c, is_multi, is_reg, ps=None, xtra_fc=None, xtra_cut=0, \n",
    "                 custom_head=None, pretrained=True):\n",
    "        self.f,self.c,self.is_multi,self.is_reg,self.xtra_cut = f,c,is_multi,is_reg,xtra_cut\n",
    "        if xtra_fc is None: xtra_fc = [512]\n",
    "        if ps is None: ps = [0.25]*len(xtra_fc) + [0.5]\n",
    "        self.ps,self.xtra_fc = ps,xtra_fc\n",
    "\n",
    "        if f in model_meta: cut,self.lr_cut = model_meta[f]\n",
    "        else: cut,self.lr_cut = 0,0\n",
    "        cut-=xtra_cut\n",
    "        layers = cut_model(f(pretrained), cut)\n",
    "        \n",
    "        #replace first convolutional layer by 4->64 while keeping corresponding weights\n",
    "        #and initializing new weights with zeros\n",
    "        #####################################################\n",
    "        w = layers[0].weight\n",
    "        layers[0] = nn.Conv2d(4,64,kernel_size=(7,7),stride=(2,2),padding=(3, 3), bias=False)\n",
    "        layers[0].weight = torch.nn.Parameter(torch.cat((w,torch.zeros(64,1,7,7)),dim=1))\n",
    "        #####################################################\n",
    "        \n",
    "        self.nf = model_features[f] if f in model_features else (num_features(layers)*2)\n",
    "        if not custom_head: layers += [AdaptiveConcatPool2d(), Flatten()]\n",
    "        self.top_model = nn.Sequential(*layers)\n",
    "\n",
    "        n_fc = len(self.xtra_fc)+1\n",
    "        if not isinstance(self.ps, list): self.ps = [self.ps]*n_fc\n",
    "\n",
    "        if custom_head: fc_layers = [custom_head]\n",
    "        else: fc_layers = self.get_fc_layers()\n",
    "        self.n_fc = len(fc_layers)\n",
    "        self.fc_model = to_gpu(nn.Sequential(*fc_layers))\n",
    "        if not custom_head: apply_init(self.fc_model, kaiming_normal)\n",
    "        self.model = to_gpu(nn.Sequential(*(layers+fc_layers)))\n",
    "\n",
    "    @property\n",
    "    def name(self): return f'{self.f.__name__}_{self.xtra_cut}'\n",
    "\n",
    "    def create_fc_layer(self, ni, nf, p, actn=None):\n",
    "        res=[nn.BatchNorm1d(num_features=ni)]\n",
    "        if p: res.append(nn.Dropout(p=p))\n",
    "        res.append(nn.Linear(in_features=ni, out_features=nf))\n",
    "        if actn: res.append(actn)\n",
    "        return res\n",
    "\n",
    "    def get_fc_layers(self):\n",
    "        res=[]\n",
    "        ni=self.nf\n",
    "        for i,nf in enumerate(self.xtra_fc):\n",
    "            res += self.create_fc_layer(ni, nf, p=self.ps[i], actn=nn.ReLU())\n",
    "            ni=nf\n",
    "        final_actn = nn.Sigmoid() if self.is_multi else nn.LogSoftmax()\n",
    "        if self.is_reg: final_actn = None\n",
    "        res += self.create_fc_layer(ni, self.c, p=self.ps[-1], actn=final_actn)\n",
    "        return res\n",
    "\n",
    "    def get_layer_groups(self, do_fc=False):\n",
    "        if do_fc:\n",
    "            return [self.fc_model]\n",
    "        idxs = [self.lr_cut]\n",
    "        c = children(self.top_model)\n",
    "        if len(c)==3: c = children(c[0])+c[1:]\n",
    "        lgs = list(split_by_idxs(c,idxs))\n",
    "        return lgs+[self.fc_model]\n",
    "    \n",
    "class ConvLearner(Learner):\n",
    "    def __init__(self, data, models, precompute=False, **kwargs):\n",
    "        self.precompute = False\n",
    "        super().__init__(data, models, **kwargs)\n",
    "        if hasattr(data, 'is_multi') and not data.is_reg and self.metrics is None:\n",
    "            self.metrics = [accuracy_thresh(0.5)] if self.data.is_multi else [accuracy]\n",
    "        if precompute: self.save_fc1()\n",
    "        self.freeze()\n",
    "        self.precompute = precompute\n",
    "\n",
    "    def _get_crit(self, data):\n",
    "        if not hasattr(data, 'is_multi'): return super()._get_crit(data)\n",
    "\n",
    "        return F.l1_loss if data.is_reg else F.binary_cross_entropy if data.is_multi else F.nll_loss\n",
    "\n",
    "    @classmethod\n",
    "    def pretrained(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n",
    "                   pretrained=True, **kwargs):\n",
    "        models = ConvnetBuilder_custom(f, data.c, data.is_multi, data.is_reg,\n",
    "            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=pretrained)\n",
    "        return cls(data, models, precompute, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def lsuv_learner(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n",
    "                  needed_std=1.0, std_tol=0.1, max_attempts=10, do_orthonorm=False, **kwargs):\n",
    "        models = ConvnetBuilder(f, data.c, data.is_multi, data.is_reg,\n",
    "            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=False)\n",
    "        convlearn=cls(data, models, precompute, **kwargs)\n",
    "        convlearn.lsuv_init()\n",
    "        return convlearn\n",
    "    \n",
    "    @property\n",
    "    def model(self): return self.models.fc_model if self.precompute else self.models.model\n",
    "    \n",
    "    def half(self):\n",
    "        if self.fp16: return\n",
    "        self.fp16 = True\n",
    "        if type(self.model) != FP16: self.models.model = FP16(self.model)\n",
    "        if not isinstance(self.models.fc_model, FP16): self.models.fc_model = FP16(self.models.fc_model)\n",
    "    def float(self):\n",
    "        if not self.fp16: return\n",
    "        self.fp16 = False\n",
    "        if type(self.models.model) == FP16: self.models.model = self.model.module.float()\n",
    "        if type(self.models.fc_model) == FP16: self.models.fc_model = self.models.fc_model.module.float()\n",
    "\n",
    "    @property\n",
    "    def data(self): return self.fc_data if self.precompute else self.data_\n",
    "\n",
    "    def create_empty_bcolz(self, n, name):\n",
    "        return bcolz.carray(np.zeros((0,n), np.float32), chunklen=1, mode='w', rootdir=name)\n",
    "\n",
    "    def set_data(self, data, precompute=False):\n",
    "        super().set_data(data)\n",
    "        if precompute:\n",
    "            self.unfreeze()\n",
    "            self.save_fc1()\n",
    "            self.freeze()\n",
    "            self.precompute = True\n",
    "        else:\n",
    "            self.freeze()\n",
    "\n",
    "    def get_layer_groups(self):\n",
    "        return self.models.get_layer_groups(self.precompute)\n",
    "\n",
    "    def summary(self):\n",
    "        precompute = self.precompute\n",
    "        self.precompute = False\n",
    "        res = super().summary()\n",
    "        self.precompute = precompute\n",
    "        return res\n",
    "\n",
    "    def get_activations(self, force=False):\n",
    "        tmpl = f'_{self.models.name}_{self.data.sz}.bc'\n",
    "        # TODO: Somehow check that directory names haven't changed (e.g. added test set)\n",
    "        names = [os.path.join(self.tmp_path, p+tmpl) for p in ('x_act', 'x_act_val', 'x_act_test')]\n",
    "        if os.path.exists(names[0]) and not force:\n",
    "            self.activations = [bcolz.open(p) for p in names]\n",
    "        else:\n",
    "            self.activations = [self.create_empty_bcolz(self.models.nf,n) for n in names]\n",
    "\n",
    "    def save_fc1(self):\n",
    "        self.get_activations()\n",
    "        act, val_act, test_act = self.activations\n",
    "        m=self.models.top_model\n",
    "        if len(self.activations[0])!=len(self.data.trn_ds):\n",
    "            predict_to_bcolz(m, self.data.fix_dl, act)\n",
    "        if len(self.activations[1])!=len(self.data.val_ds):\n",
    "            predict_to_bcolz(m, self.data.val_dl, val_act)\n",
    "        if self.data.test_dl and (len(self.activations[2])!=len(self.data.test_ds)):\n",
    "            if self.data.test_dl: predict_to_bcolz(m, self.data.test_dl, test_act)\n",
    "\n",
    "        self.fc_data = ImageClassifierData.from_arrays(self.data.path,\n",
    "                (act, self.data.trn_y), (val_act, self.data.val_y), self.data.bs, classes=self.data.classes,\n",
    "                test = test_act if self.data.test_dl else None, num_workers=8)\n",
    "\n",
    "    def freeze(self):\n",
    "        self.freeze_to(-1)\n",
    "\n",
    "    def unfreeze(self):\n",
    "        self.freeze_to(0)\n",
    "        self.precompute = False\n",
    "\n",
    "    def predict_array(self, arr):\n",
    "        precompute = self.precompute\n",
    "        self.precompute = False\n",
    "        pred = super().predict_array(arr)\n",
    "        self.precompute = precompute\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SE Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_settings = {\n",
    "    'se_resnext50_32x4d': {\n",
    "        'imagenet': {\n",
    "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth',\n",
    "            'input_space': 'RGB',\n",
    "            'input_size': [3, 224, 224],\n",
    "            'input_range': [0, 1],\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225],\n",
    "            'num_classes': 1000\n",
    "        }\n",
    "    },\n",
    "    'se_resnext101_32x4d': {\n",
    "        'imagenet': {\n",
    "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth',\n",
    "            'input_space': 'RGB',\n",
    "            'input_size': [3, 224, 224],\n",
    "            'input_range': [0, 1],\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225],\n",
    "            'num_classes': 1000\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "class SEModule(nn.Module):\n",
    "    def __init__(self, channels, reduction):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        module_input = x\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return module_input * x\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for bottlenecks that implements `forward()` method.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out = self.se_module(out) + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    Bottleneck for SENet154.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes * 2)\n",
    "        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n",
    "                               stride=stride, padding=1, groups=groups,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes * 4)\n",
    "        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "class SEResNetBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n",
    "    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n",
    "    (the latter is used in the torchvision implementation of ResNet).\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None):\n",
    "        super(SEResNetBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n",
    "                               stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n",
    "                               groups=groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "class SEResNeXtBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None, base_width=4):\n",
    "        super(SEResNeXtBottleneck, self).__init__()\n",
    "        width = math.floor(planes * (base_width / 64)) * groups\n",
    "        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n",
    "                               stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n",
    "                               padding=1, groups=groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.drop1 = nn.Dropout2d(0.2)\n",
    "        self.drop2 = nn.Dropout2d(0.2)\n",
    "\n",
    "class SENet(nn.Module):\n",
    "    def __init__(self, block, layers, groups, reduction, dropout_p=0.5,\n",
    "                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n",
    "                 downsample_padding=1, num_classes=1000):\n",
    "        super(SENet, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        if input_3x3:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(64)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn2', nn.BatchNorm2d(64)),\n",
    "                ('relu2', nn.ReLU(inplace=True)),\n",
    "                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn3', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu3', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        else:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n",
    "                                    padding=3, bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        # To preserve compatibility with Caffe weights `ceil_mode=True`\n",
    "        # is used instead of `padding=1`.\n",
    "        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n",
    "                                                    ceil_mode=True)))\n",
    "        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n",
    "        self.layer1 = self._make_layer(\n",
    "            block,\n",
    "            planes=64,\n",
    "            blocks=layers[0],\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=1,\n",
    "            downsample_padding=0\n",
    "        )\n",
    "        self.layer2 = self._make_layer(\n",
    "            block,\n",
    "            planes=128,\n",
    "            blocks=layers[1],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block,\n",
    "            planes=256,\n",
    "            blocks=layers[2],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block,\n",
    "            planes=512,\n",
    "            blocks=layers[3],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n",
    "        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "#         self.last_linear = nn.Linear(512*16, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n",
    "                    downsample_kernel_size=1, downsample_padding=0):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=downsample_kernel_size, stride=stride,\n",
    "                          padding=downsample_padding, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n",
    "                            downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups, reduction))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def initialize_pretrained_model(model, num_classes, settings):\n",
    "    assert num_classes == settings['num_classes'], \\\n",
    "        'num_classes should be {}, but is {}'.format(\n",
    "            settings['num_classes'], num_classes)\n",
    "    model.load_state_dict(model_zoo.load_url(settings['url']))\n",
    "    model.input_space = settings['input_space']\n",
    "    model.input_size = settings['input_size']\n",
    "    model.input_range = settings['input_range']\n",
    "    model.mean = settings['mean']\n",
    "    model.std = settings['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNextConvBuilder(ConvnetBuilder_custom):\n",
    "    def __init__(self, f, c, is_multi, is_reg, ps=None, xtra_fc=None, xtra_cut=0, \n",
    "                 custom_head=None, pretrained=True):\n",
    "        self.f,self.c,self.is_multi,self.is_reg,self.xtra_cut = f,c,is_multi,is_reg,xtra_cut\n",
    "        if xtra_fc is None: xtra_fc = [512]\n",
    "        if ps is None: ps = [0.25]*len(xtra_fc) + [0.5]\n",
    "        self.ps,self.xtra_fc = ps,xtra_fc\n",
    "\n",
    "        if f in model_meta: cut,self.lr_cut = model_meta[f]\n",
    "        else: cut,self.lr_cut = 8,6\n",
    "        cut-=xtra_cut\n",
    "        layers = cut_model(f(), 5)\n",
    "        \n",
    "        #replace first convolutional layer by 4->64 while keeping corresponding weights\n",
    "        #and initializing new weights with zeros\n",
    "        #####################################################\n",
    "        w = layers[0].conv1.weight\n",
    "        w1 = layers[0].conv1.weight[:,0].unsqueeze(dim=1)\n",
    "        layers[0].conv1 = nn.Conv2d(4,64,kernel_size=(7,7),stride=(2,2),padding=(3, 3), bias=False)\n",
    "        layers[0].conv1.weight = torch.nn.Parameter(torch.cat((w,w1),dim=1))\n",
    "        #####################################################\n",
    "        \n",
    "        self.nf = model_features[f] if f in model_features else (num_features(layers)*2)\n",
    "        if not custom_head: layers += [AdaptiveConcatPool2d(), Flatten()]\n",
    "        self.top_model = nn.Sequential(*layers)\n",
    "\n",
    "        n_fc = len(self.xtra_fc)+1\n",
    "        if not isinstance(self.ps, list): self.ps = [self.ps]*n_fc\n",
    "\n",
    "        if custom_head: fc_layers = [custom_head]\n",
    "        else: fc_layers = self.get_fc_layers()\n",
    "        self.n_fc = len(fc_layers)\n",
    "        self.fc_model = to_gpu(nn.Sequential(*fc_layers))\n",
    "        if not custom_head: apply_init(self.fc_model, kaiming_normal)\n",
    "        self.model = to_gpu(nn.Sequential(*(layers+fc_layers)))\n",
    "\n",
    "class ResNextConv(ConvLearner):\n",
    "        @classmethod\n",
    "        def pretrained(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n",
    "                   pretrained=True, **kwargs):\n",
    "            models = ResNextConvBuilder(f, data.c, data.is_multi, data.is_reg,\n",
    "            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=pretrained)\n",
    "            return cls(data, models, precompute, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n",
    "    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n",
    "                  dropout_p=0.5, inplanes=64, input_3x3=False,\n",
    "                  downsample_kernel_size=1, downsample_padding=0,\n",
    "                  num_classes=num_classes)\n",
    "    if pretrained is not None:\n",
    "        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n",
    "        initialize_pretrained_model(model, num_classes, settings)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold(sz, bs, k):\n",
    "    aug_tfms = [RandomRotate(30, tfm_y=TfmType.NO),\n",
    "                RandomDihedral(tfm_y=TfmType.NO),\n",
    "                RandomLighting(0.05, 0.05, tfm_y=TfmType.NO)]\n",
    "    \n",
    "    stats = A([0.00505, 0.00331, 0.00344, 0.00519], [0.10038, 0.08131, 0.08284, 0.10179])\n",
    "    tfms = tfms_from_stats(stats, sz, crop_type=CropType.NO, tfm_y=TfmType.NO, \n",
    "                aug_tfms=aug_tfms)\n",
    "    \n",
    "    trn_x = list(train_folds[k]['Id'])\n",
    "    val_x = list(val_folds[k]['Id'])\n",
    "    \n",
    "    if len(trn_x)%bs == 0:\n",
    "        ds = ImageData.get_ds(pdFilesDataset, (trn_x,TRAIN), \n",
    "                (val_x,TRAIN), tfms, test=(test_names,TEST))\n",
    "    else:\n",
    "        ds = ImageData.get_ds(pdFilesDataset, (trn_x[:-(len(trn_x)%bs)],TRAIN), \n",
    "                (val_x,TRAIN), tfms, test=(test_names,TEST))\n",
    "    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnext50_model(md):\n",
    "    learn = ResNextConv.pretrained(se_resnext50_32x4d, md, ps=0.5) #dropout 50%\n",
    "    learn.opt_fn = optim.Adam\n",
    "    learn.crit = FocalLoss()\n",
    "    learn.metrics = [acc, f1_metric]\n",
    "    learn.clip = 1.0\n",
    "    return learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_model(sz, bs, k):\n",
    "    md = get_fold(sz,bs,k)\n",
    "    learn = get_resnext50_model(md)\n",
    "    learn.freeze_to(1)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample Edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_over_fold(sz, bs, k):\n",
    "    aug_tfms = [RandomRotate(30, tfm_y=TfmType.NO),\n",
    "                RandomDihedral(tfm_y=TfmType.NO),\n",
    "                RandomLighting(0.05, 0.05, tfm_y=TfmType.NO)]\n",
    "    \n",
    "    stats = A([0.00505, 0.00331, 0.00344, 0.00519], [0.10038, 0.08131, 0.08284, 0.10179])\n",
    "    tfms = tfms_from_stats(stats, sz, crop_type=CropType.NO, tfm_y=TfmType.NO, \n",
    "                aug_tfms=aug_tfms)\n",
    "    \n",
    "    trn_x = list(over_train_folds[k]['Id'])\n",
    "    val_x = list(over_val_folds[k]['Id'])\n",
    "    \n",
    "    if len(trn_x)%bs == 0:\n",
    "        ds = ImageData.get_ds(pdFilesDataset, (trn_x,TRAIN), \n",
    "                (val_x,TRAIN), tfms, test=(test_names,TEST))\n",
    "    else:\n",
    "        ds = ImageData.get_ds(pdFilesDataset, (trn_x[:-(len(trn_x)%bs)],TRAIN), \n",
    "                (val_x,TRAIN), tfms, test=(test_names,TEST))\n",
    "    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n",
    "    return md\n",
    "\n",
    "def get_overfold_model(sz, bs, k):\n",
    "    md = get_over_fold(sz,bs,k)\n",
    "    learn = get_resnext50_model(md)\n",
    "    learn.freeze_to(1)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfold Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "wd = 1e-7\n",
    "nw=6\n",
    "lrs=np.array([lr/10,lr/3,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n",
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44174ee2fa104208bd8a6b34dc2cacfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   acc        f1_metric         \n",
      "    0      2.894141   1.894364   0.928073   1.894287  \n",
      "    1      1.952773   1.655329   0.931392   1.596589          \n",
      "    2      1.72733    1.555247   0.934232   1.658985          \n",
      "    3      1.606527   1.537318   0.934829   1.558331          \n",
      "    4      1.499816   1.553119   0.935476   1.48484           \n",
      "    5      1.39727    1.567557   0.933891   1.442354          \n",
      "    6      1.312646   1.592852   0.932178   1.413704          \n",
      "    7      1.277144   1.582182   0.931655   1.410893          \n",
      "    8      1.274289   1.574093   0.932425   1.40843           \n",
      "    9      1.240694   1.591768   0.932079   1.400213          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sz = 128\n",
    "bs = 24\n",
    "\n",
    "learn = get_overfold_model(sz,bs,3)\n",
    "learn.unfreeze()\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=10, use_clr_beta=(10,10,0.85,0.9), use_wd_sched=True)\n",
    "learn.save('SEResNextFold_128_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n",
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b012bb589c4a678bc0b82b5e09285c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   acc        f1_metric         \n",
      "    0      1.287492   1.480285   0.933829   1.411398  \n",
      "    1      1.190252   1.432211   0.936756   1.385831          \n",
      "    2      1.07842    1.427014   0.938588   1.354518          \n",
      "    3      1.050972   1.409981   0.939156   1.326916          \n",
      "    4      0.941893   1.369003   0.942046   1.312287           \n",
      "    5      0.877585   1.367926   0.942392   1.289273           \n",
      "    6      0.863571   1.329484   0.944335   1.286037           \n",
      "    7      0.830794   1.270952   0.94522    1.283957           \n",
      "    8      0.795969   1.318031   0.945266   1.267977           \n",
      "    9      0.800551   1.284401   0.945784   1.275932           \n"
     ]
    }
   ],
   "source": [
    "sz = 256\n",
    "bs = 16\n",
    "\n",
    "learn = get_overfold_model(sz,bs,3)\n",
    "learn.load('SEResNextFold_128_3')\n",
    "learn.unfreeze()\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=10, use_clr_beta=(20,8,0.85,0.9), use_wd_sched=True)\n",
    "learn.save('SEResNextFold_256_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n",
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae25459c68847b9b190579b187de4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   acc        f1_metric          \n",
      "    0      0.936123   1.493907   0.939811   1.257735  \n",
      "    1      0.909803   1.498809   0.942874   1.244564           \n",
      "    2      0.909558   1.519219   0.943788   1.222911           \n",
      "    3      0.874937   1.512659   0.94534    1.212539           \n",
      "    4      0.770423   1.367456   0.948469   1.219535           \n",
      "    5      0.801867   1.5168     0.947616   1.199073           \n",
      "    6      0.761069   1.317298   0.951853   1.197406           \n",
      "    7      0.714581   1.306861   0.951408   1.199208           \n",
      "    8      0.699356   1.419535   0.953409   1.186518           \n",
      "    9      0.662387   1.282365   0.955352   1.191486           \n",
      "    10     0.656751   1.251562   0.953623   1.178844           \n",
      "    11     0.659572   1.137671   0.956575   1.189399           \n",
      "    12     0.6996     1.205377   0.957271   1.179339           \n",
      "    13     0.662278   1.083555   0.958654   1.178905           \n",
      "    14     0.626937   1.007507   0.959074   1.192847           \n",
      "    15     0.644517   1.043043   0.959152   1.186507           \n",
      "    16     0.58247    1.004871   0.959284   1.193306           \n",
      "    17     0.61452    1.030948   0.959741   1.180298           \n",
      "    18     0.585136   1.036684   0.960284   1.175784           \n",
      "    19     0.565189   1.011318   0.960708   1.180129           \n"
     ]
    }
   ],
   "source": [
    "sz = 512\n",
    "bs = 8\n",
    "\n",
    "learn = get_overfold_model(sz,bs,3)\n",
    "learn.load('SEResNextFold_256_3')\n",
    "learn.unfreeze()\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=20, use_clr_beta=(20,8,0.85,0.9), use_wd_sched=True, best_save_name='SEResNextFold_512_3best')\n",
    "learn.save('SEResNextFold_512_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    }
   ],
   "source": [
    "pred = make_prediction()\n",
    "save_pred(pred, th_t, fname=f'protein_class_3_oversample_tht.csv')\n",
    "save_pred(pred, 0.5, fname=f'protein_class_3_oversample_0.5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnext50_model(md):\n",
    "    learn = ResNextConv.pretrained(se_resnext50_32x4d, md, ps=0.5) #dropout 50%\n",
    "    learn.opt_fn = optim.Adam\n",
    "    learn.crit = FocalLoss()\n",
    "    learn.metrics = [acc, f1_metric]\n",
    "    learn.clip = 0.1 \n",
    "    return learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_model(sz, bs, k):\n",
    "    md = get_fold(sz,bs,k)\n",
    "    learn = get_resnext50_model(md)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "wd = 1e-7\n",
    "nw = 6\n",
    "lrs=np.array([lr/10,lr/3,lr])\n",
    "\n",
    "for k in range(folds):\n",
    "    # 128x128 phase\n",
    "    learn = get_fold_model(128, 24, k)\n",
    "    \n",
    "    learn.fit(lr,1,wds=wd,cycle_len=8,use_clr=(5,8))\n",
    "\n",
    "    learn.unfreeze()\n",
    "    learn.bn_freeze(True)\n",
    "\n",
    "    learn.fit(lrs/2, 1, wds=wd, cycle_len=10,use_clr=(20,10))\n",
    "\n",
    "    learn.save(f'128_SeResNext50_{k}')\n",
    "\n",
    "    # 256 x 256 phase\n",
    "    learn = get_fold_model(256, 16, k)\n",
    "    learn.load(f'128_SeResNext50_{k}')\n",
    "\n",
    "    learn.fit(lr,1,wds=wd, cycle_len=5,use_clr=(5,5))\n",
    "\n",
    "    learn.unfreeze()\n",
    "    learn.bn_freeze(True)\n",
    "\n",
    "    learn.fit(lrs/2,2,wds=wd, cycle_len=10,use_clr=(20,8))\n",
    "\n",
    "    learn.save(f'256_SeResNext50_{k}')\n",
    "\n",
    "    #512 x 512 phase\n",
    "    learn = get_fold_model(512, 8, k)\n",
    "    learn.load(f'256_SeResNext50_{k}')\n",
    "\n",
    "    learn.fit(lr,1, wds=wd, cycle_len=2,use_clr=(5,4))\n",
    "\n",
    "    learn.unfreeze()\n",
    "    learn.bn_freeze(True)\n",
    "\n",
    "    learn.fit(lrs/3, 2, wds=wd,cycle_len=16,use_clr=(20,8))\n",
    "    \n",
    "    learn.fit(lrs/8, 1, wds=wd,cycle_len=9,use_clr=(20,8))\n",
    "\n",
    "    learn.save(f'512_SeResNext50_ph1_{k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnext50_model(md):\n",
    "    learn = ResNextConv.pretrained(se_resnext50_32x4d, md, ps=0.5) #dropout 50%\n",
    "    learn.opt_fn = optim.Adam\n",
    "    learn.crit = FocalLoss()\n",
    "    learn.metrics = [acc, f1_metric]\n",
    "    learn.clip = 0.1 \n",
    "    return learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_model(sz, bs, k):\n",
    "    md = get_fold(sz,bs,k)\n",
    "    learn = get_resnext50_model(md)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "wd = 1e-7\n",
    "lrs=np.array([lr/10,lr/3,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 128\n",
    "bs = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n",
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f482c54e40847bd970b73095390c0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   acc        f1_metric         \n",
      "    0      1.359387   58.925576  0.924173   0.458162  \n",
      "    1      1.427781   11266.822322 0.856858   1.045355        \n",
      "  6%|         | 93/1542 [00:18<04:41,  5.15it/s, loss=1.42]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-dc19d2f3f15d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fold_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_clr_beta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.85\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_wd_sched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'SEResNextFold_{sz}_{k}_gc0.12'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, visualize, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_stepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y, epoch)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mIS_TORCH_04\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_params_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_params_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k in range(folds):\n",
    "    learn = get_fold_model(sz, bs, k)\n",
    "    learn.unfreeze()\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=10, use_clr_beta=(10,10,0.85,0.9), use_wd_sched=True)\n",
    "    \n",
    "    pa = f'SEResNextFold_{sz}_{k}_gc0.12'\n",
    "    learn.save(pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 256\n",
    "bs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(folds):\n",
    "    learn = get_fold_model(sz, bs, k)\n",
    "    learn.load(f'SEResNextFold_128_{k}_gc0.12')\n",
    "    learn.unfreeze()\n",
    "    \n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=30, use_clr_beta=(20,8,0.85,0.9), use_wd_sched=True)\n",
    "    \n",
    "    pa = f'SEResNextFold_256_{k}_gc0.12'\n",
    "    learn.save(pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 512\n",
    "bs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(folds):\n",
    "    learn = get_fold_model(sz, bs, k)\n",
    "    learn.load(f'SEResNextFold_256_{k}_gc0.12')\n",
    "    learn.unfreeze()\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=19, use_clr_beta=(20,8,0.85,0.9), use_wd_sched=True)\n",
    "    \n",
    "    pa = f'SEResNextFold_512_{k}_gc0.12'\n",
    "    learn.save(pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_array = np.zeros((11702, 28),dtype=float)\n",
    "\n",
    "for k in range(folds):\n",
    "    learn.load(f'SEResNextFold_512_{k}_gc0.12')\n",
    "    preds_t,y_t = learn.TTA(n_aug=8, is_test=True)\n",
    "    preds_t = np.stack(preds_t, axis=-1)\n",
    "    preds_t = sigmoid_np(preds_t)\n",
    "    pred_t = preds_t.max(axis=-1)\n",
    "    pred_array = pred_t + pred_array\n",
    "\n",
    "pred_array = pred_array / folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pred(pred_array,th_t,'protein_5folds_gc12.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Oversampling Edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_over_fold(sz, bs, k):\n",
    "    aug_tfms = [RandomRotate(30, tfm_y=TfmType.NO),\n",
    "                RandomDihedral(tfm_y=TfmType.NO),\n",
    "                RandomLighting(0.05, 0.05, tfm_y=TfmType.NO)]\n",
    "    \n",
    "    stats = A([0.00505, 0.00331, 0.00344, 0.00519], [0.10038, 0.08131, 0.08284, 0.10179])\n",
    "    tfms = tfms_from_stats(stats, sz, crop_type=CropType.NO, tfm_y=TfmType.NO, \n",
    "                aug_tfms=aug_tfms)\n",
    "    \n",
    "    trn_x = list(over_train_folds[k]['Id'])\n",
    "    val_x = list(over_val_folds[k]['Id'])\n",
    "    \n",
    "    if len(trn_x)%bs == 0:\n",
    "        ds = ImageData.get_ds(pdFilesDataset, (trn_x,TRAIN), \n",
    "                (val_x,TRAIN), tfms, test=(test_names,TEST))\n",
    "    else:\n",
    "        ds = ImageData.get_ds(pdFilesDataset, (trn_x[:-(len(trn_x)%bs)],TRAIN), \n",
    "                (val_x,TRAIN), tfms, test=(test_names,TEST))\n",
    "    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n",
    "    return md\n",
    "\n",
    "def get_overfold_model(sz, bs, k):\n",
    "    md = get_over_fold(sz,bs,k)\n",
    "    learn = get_resnext50_model(md)\n",
    "    learn.freeze_to(1)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find fold on 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "wd = 1e-7\n",
    "lrs=np.array([lr/10,lr/3,lr])\n",
    "\n",
    "sz = 256\n",
    "bs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n",
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761b85d4823941ba871266accf95b298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   acc        f1_metric         \n",
      "    0      1.328593   1.272191   0.947765   1.413501  \n",
      "    1      1.137745   1.023102   0.953469   1.405513          \n",
      "    2      1.043368   0.915269   0.958118   1.378022           \n",
      "    3      1.026085   0.913677   0.958258   1.342311           \n",
      "    4      0.936701   0.878641   0.959737   1.312736           \n",
      "    5      0.887632   0.79616    0.962896   1.300051           \n",
      "    6      0.833138   0.764952   0.963968   1.287958           \n",
      "    7      0.811208   0.743725   0.965162   1.277324           \n",
      "    8      0.792274   0.738354   0.965039   1.274591           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n",
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3897b98c8c9642b084b9f1ea0a4ae350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   acc        f1_metric         \n",
      "    0      1.366687   1.322389   0.947173   1.403577  \n",
      "    1      1.121734   1.016569   0.953633   1.372699          \n",
      "    2      1.036949   0.918403   0.957367   1.365476          \n",
      "    3      0.974304   0.850032   0.960168   1.354051           \n",
      "    4      0.975429   0.797373   0.962619   1.327153           \n",
      "    5      0.891206   0.776034   0.96365    1.306319           \n",
      "    6      0.87218    0.732663   0.965202   1.318009           \n",
      "    7      0.854386   0.709041   0.966113   1.282925           \n",
      "    8      0.798978   0.69504    0.96668    1.284465           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n",
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4330f48bc09b48d9a1bc53c4095a428a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   acc        f1_metric         \n",
      "    0      1.320795   1.318231   0.94644    1.40908   \n",
      "    1      1.096629   1.049087   0.953805   1.348498          \n",
      "    2      1.02817    0.943915   0.956842   1.342956          \n",
      "    3      0.993916   0.930248   0.957847   1.341957           \n",
      "    4      0.902335   0.867526   0.959587   1.322497           \n",
      "    5      0.919534   0.817346   0.962371   1.311022           \n",
      "    6      0.833776   0.79827    0.963388   1.269858           \n",
      "    7      0.81283    0.758793   0.964842   1.269543           \n",
      "    8      0.790321   0.748633   0.965229   1.267038           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n",
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e1cb0886e64f53be029be842863eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   acc        f1_metric         \n",
      "    0      1.349492   1.380949   0.944162   1.383587  \n",
      "    1      1.13033    0.991769   0.954015   1.392682          \n",
      "    2      1.005106   0.874963   0.958865   1.366259          \n",
      "    3      1.013256   0.844407   0.960629   1.335342           \n",
      "    4      0.960734   0.797777   0.962704   1.341817           \n",
      "    5      0.886598   0.751285   0.96478    1.298729           \n",
      "    6      0.879901   0.737562   0.964874   1.2824             \n",
      "    7      0.811503   0.692786   0.967278   1.291397           \n",
      "    8      0.80585    0.703499   0.966596   1.271528           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n",
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc06fb70a9b0407abc683f8d837fd7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   acc        f1_metric         \n",
      "    0      1.348071   1.363003   0.949994   1.32488   \n",
      "    1      1.132455   0.967116   0.955429   1.395789          \n",
      "    2      1.016522   0.870074   0.960179   1.33425           \n",
      "    3      1.026964   0.846754   0.960929   1.366453           \n",
      "    4      0.931521   0.807102   0.962607   1.330315           \n",
      "    5      0.92518    0.749328   0.964548   1.320289           \n",
      "    6      0.852273   0.732965   0.965661   1.281307           \n",
      "    7      0.841811   0.790707   0.967464   1.312267           \n",
      "    8      0.799584   0.779458   0.967649   1.286633           \n"
     ]
    }
   ],
   "source": [
    "for k in range(folds):\n",
    "    md = get_fold(sz,bs,k)\n",
    "    learn = get_resnext50_model(md)\n",
    "    \n",
    "    learn.unfreeze()\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=9, use_clr_beta=(10,10,0.85,0.9), use_wd_sched=True)\n",
    "    \n",
    "    pa = f'SEResNextFold_{sz}_{k}_1cycle'\n",
    "    learn.save(pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 512\n",
    "bs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(folds):\n",
    "    md = get_fold(sz,bs,k)\n",
    "    learn = get_resnext50_model(md)\n",
    "    \n",
    "    learn.load(f'SEResNextFold_256_{k}_1cycle')\n",
    "    learn.unfreeze()\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=9, use_clr_beta=(10,10,0.85,0.9), use_wd_sched=True)\n",
    "    pa = f'SEResNextFold_{sz}_{k}_1cycle'\n",
    "    \n",
    "    learn.save(pa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(folds):\n",
    "    # 128x128\n",
    "    learn = get_fold_model(128, 24, k)\n",
    "    lead.load(f'SEResNextFold_128_{k}')\n",
    "    \n",
    "    learn.fit(lr, 1, wds=wd, cycle_len=8, use_clr_beta=(5,4,0.85,0.9), use_wd_sched=True)\n",
    "    \n",
    "    learn.unfreeze()\n",
    "    learn.bn_freeze(True)\n",
    "    \n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=10, use_clr_beta=(5,4,0.85,0.9), use_wd_sched=True)\n",
    "    \n",
    "    learn.save(f'SEResNextFold_256_{k}')\n",
    "    \n",
    "    #256x256\n",
    "    learn = get_fold_model(256, 16, k)\n",
    "    learn.load(f'SEResNextFold_256_{k}')\n",
    "    \n",
    "    learn.fit(lr, 1, wds=wd, cycle_len=5, use_clr_beta=(5,5,0.85,0.9), use_wd_sched=True)\n",
    "\n",
    "    learn.unfreeze()\n",
    "    learn.bn_freeze(True)\n",
    "    \n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=20, use_clr_beta=(20,8,0.85,0.9), use_wd_sched=True)\n",
    "\n",
    "    learn.save(f'SeResNext50_256_{k}')\n",
    "    \n",
    "    #512 x 512 phase\n",
    "    learn = get_fold_model(512, 8, k)\n",
    "    learn.load(f'SeResNext50_256_{k}')\n",
    "\n",
    "    learn.fit(lr,1, wds=wd, cycle_len=2,use_clr=(5,4))\n",
    "\n",
    "    learn.unfreeze()\n",
    "    learn.bn_freeze(True)\n",
    "\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=30, use_clr_beta=(20,8,0.85,0.9), use_wd_sched=True)\n",
    "\n",
    "    learn.save(f'SeResNext50_512_{k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_t = np.array([0.565,0.39,0.55,0.345,0.33,0.39,0.33,0.45,0.38,0.39,\n",
    "               0.34,0.42,0.31,0.38,0.49,0.50,0.38,0.43,0.46,0.40,\n",
    "               0.39,0.505,0.37,0.47,0.41,0.545,0.32,0.1])\n",
    "\n",
    "def make_prediction():\n",
    "    preds,y = learn.TTA(n_aug=8, is_test=True)\n",
    "    preds = np.stack(preds, axis=-1)\n",
    "    preds = sigmoid_np(preds)\n",
    "    pred = preds.max(axis=-1)\n",
    "    return pred\n",
    "\n",
    "def save_pred(pred, th=0.5, fname='protein_classification.csv'):\n",
    "    pred_list = []\n",
    "    for line in pred:\n",
    "        s = ' '.join(list([str(i) for i in np.nonzero(line>th)[0]]))\n",
    "        pred_list.append(s)\n",
    "        \n",
    "    sample_df = pd.read_csv(SAMPLE)\n",
    "    sample_list = list(sample_df.Id)\n",
    "    pred_dic = dict((key, value) for (key, value) \n",
    "                in zip(learn.data.test_ds.fnames,pred_list))\n",
    "    pred_list_cor = [pred_dic[id] for id in sample_list]\n",
    "    df = pd.DataFrame({'Id':sample_list,'Predicted':pred_list_cor})\n",
    "    df.to_csv(fname, header=True, index=False)\n",
    "\n",
    "def sigmoid_np(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
