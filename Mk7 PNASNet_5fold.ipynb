{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "from fastai.dataset import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './'\n",
    "TRAIN = 'data/train/'\n",
    "TEST = 'data/test/'\n",
    "LABELS = 'data/train.csv'\n",
    "SAMPLE = 'data/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = list({f[:36] for f in os.listdir(TRAIN)})\n",
    "test_names = list({f[:36] for f in os.listdir(TEST)})\n",
    "tr_n, val_n = train_test_split(train_names, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_rgby(path,id): \n",
    "    colors = ['red','green','blue','yellow']\n",
    "    flags = cv2.IMREAD_GRAYSCALE\n",
    "    img = [cv2.imread(os.path.join(path, id+'_'+color+'.png'), flags).astype(np.float32)/255\n",
    "           for color in colors]\n",
    "    return np.stack(img, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_label_dict = {\n",
    "0:  'Nucleoplasm',\n",
    "1:  'Nuclear membrane',\n",
    "2:  'Nucleoli',   \n",
    "3:  'Nucleoli fibrillar center',\n",
    "4:  'Nuclear speckles',\n",
    "5:  'Nuclear bodies',\n",
    "6:  'Endoplasmic reticulum',   \n",
    "7:  'Golgi apparatus',\n",
    "8:  'Peroxisomes',\n",
    "9:  'Endosomes',\n",
    "10:  'Lysosomes',\n",
    "11:  'Intermediate filaments',\n",
    "12:  'Actin filaments',\n",
    "13:  'Focal adhesion sites',   \n",
    "14:  'Microtubules',\n",
    "15:  'Microtubule ends',  \n",
    "16:  'Cytokinetic bridge',   \n",
    "17:  'Mitotic spindle',\n",
    "18:  'Microtubule organizing center',  \n",
    "19:  'Centrosome',\n",
    "20:  'Lipid droplets',\n",
    "21:  'Plasma membrane',   \n",
    "22:  'Cell junctions', \n",
    "23:  'Mitochondria',\n",
    "24:  'Aggresome',\n",
    "25:  'Cytosol',\n",
    "26:  'Cytoplasmic bodies',   \n",
    "27:  'Rods & rings' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pdFilesDataset(FilesDataset):\n",
    "    def __init__(self, fnames, path, transform):\n",
    "        self.labels = pd.read_csv(LABELS).set_index('Id')\n",
    "        self.labels['Target'] = [[int(i) for i in s.split()] for s in self.labels['Target']]\n",
    "        super().__init__(fnames, transform, path)\n",
    "    \n",
    "    def get_x(self, i):\n",
    "        img = open_rgby(self.path,self.fnames[i])\n",
    "        if self.sz == 512: return img \n",
    "        else: return cv2.resize(img, (self.sz, self.sz),cv2.INTER_AREA)\n",
    "    \n",
    "    def get_y(self, i):\n",
    "        if(self.path == TEST): return np.zeros(len(name_label_dict),dtype=np.int)\n",
    "        else:\n",
    "            labels = self.labels.loc[self.fnames[i]]['Target']\n",
    "            return np.eye(len(name_label_dict),dtype=np.float)[labels].sum(axis=0)\n",
    "        \n",
    "    @property\n",
    "    def is_multi(self): return True\n",
    "    @property\n",
    "    def is_reg(self):return True\n",
    "    #this flag is set to remove the output sigmoid that allows log(sigmoid) optimization\n",
    "    #of the numerical stability of the loss function\n",
    "    \n",
    "    def get_c(self): return len(name_label_dict) #number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sz,bs):\n",
    "    #data augmentation\n",
    "    aug_tfms = [RandomRotate(30, tfm_y=TfmType.NO),\n",
    "                RandomDihedral(tfm_y=TfmType.NO)]\n",
    "    stats = A([0.00505, 0.00331, 0.00344, 0.00519], [0.10038, 0.08131, 0.08284, 0.10179])\n",
    "    tfms = tfms_from_stats(stats, sz, crop_type=CropType.NO, tfm_y=TfmType.NO, \n",
    "                aug_tfms=aug_tfms)\n",
    "    ds = ImageData.get_ds(pdFilesDataset, (tr_n[:-(len(tr_n)%bs)],TRAIN), \n",
    "                (val_n,TRAIN), tfms, test=(test_names,TEST))\n",
    "    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n",
    "    return md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "folds = 5\n",
    "train_folds = []\n",
    "val_folds = []\n",
    "\n",
    "def make_folds():\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=33, shuffle=True)\n",
    "    \n",
    "    train_names = list({f[:36] for f in os.listdir(TRAIN)})\n",
    "    test_names = list({f[:36] for f in os.listdir(TEST)})\n",
    "    trn_df = pd.read_csv('data/train.csv')\n",
    "    \n",
    "    for train_index, evaluate_index in skf.split(trn_df.index.values, trn_df.Target):\n",
    "        trn_value = trn_df.iloc[train_index]\n",
    "        val_value = trn_df.iloc[evaluate_index]\n",
    "        train_folds.append(trn_value)\n",
    "        val_folds.append(val_value)\n",
    "        print(train_index.shape, evaluate_index.shape)\n",
    "    \n",
    "    for i in range(folds):\n",
    "        train_folds[i].to_csv(f'data/5_fold/trn_folds_{i}')\n",
    "        val_folds[i].to_csv(f'data/5_fold/val_folds_{i}')\n",
    "\n",
    "def load_folds():\n",
    "    for i in range(folds):\n",
    "        train_folds.append(pd.read_csv(f'data/5_fold/trn_folds_{i}'))\n",
    "        val_folds.append(pd.read_csv(f'data/5_fold/val_folds_{i}'))\n",
    "        print(len(train_folds[i]), len(val_folds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24673 6399\n",
      "24836 6236\n",
      "24710 6362\n",
      "24997 6075\n",
      "25072 6000\n"
     ]
    }
   ],
   "source": [
    "# make_folds()\n",
    "load_folds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "folds = 5\n",
    "over_train_folds = []\n",
    "over_val_folds = []\n",
    "\n",
    "def make_over_folds():\n",
    "    skf = StratifiedKFold(n_splits=folds, random_state=33, shuffle=True)\n",
    "    \n",
    "    trn_df = pd.read_csv('data/train.csv')\n",
    "    unique = trn_df.drop_duplicates('Target')\n",
    "    uniquex50 = pd.concat([unique]*50)\n",
    "    \n",
    "    for train_index, evaluate_index in skf.split(trn_df.index.values, trn_df.Target):\n",
    "        trn_value = trn_df.iloc[train_index]\n",
    "        val_value = trn_df.iloc[evaluate_index]\n",
    "        \n",
    "        over_train_folds.append(trn_value)\n",
    "        over_val_folds.append(val_value)\n",
    "        print(train_index.shape, evaluate_index.shape)\n",
    "    \n",
    "    for k in range(folds):\n",
    "        trn_fold_frame = over_train_folds[k].copy()\n",
    "        val_fold_frame = over_val_folds[k].copy()\n",
    "        \n",
    "        trn_unique = trn_fold_frame[~trn_fold_frame['Target'].duplicated(keep=False)]\n",
    "        val_unique = val_fold_frame[~val_fold_frame['Target'].duplicated(keep=False)]\n",
    "        \n",
    "        over_train_folds[k] = pd.concat([trn_fold_frame, uniquex50], ignore_index=True)\n",
    "        over_val_folds[k] = pd.concat([val_fold_frame], ignore_index=True)\n",
    "        \n",
    "    for i in range(folds):\n",
    "        over_train_folds[i].to_csv(f'data/5_fold/allsparse_oversample/trn_folds_{i}')\n",
    "        over_val_folds[i].to_csv(f'data/5_fold/allsparse_oversample/val_folds_{i}')\n",
    "\n",
    "def load_over_folds():\n",
    "    for i in range(folds):\n",
    "        over_train_folds.append(pd.read_csv(f'data/5_fold/allsparse_oversample/trn_folds_{i}'))\n",
    "        over_val_folds.append(pd.read_csv(f'data/5_fold/allsparse_oversample/val_folds_{i}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:626: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24673,) (6399,)\n",
      "(24836,) (6236,)\n",
      "(24710,) (6362,)\n",
      "(24997,) (6075,)\n",
      "(25072,) (6000,)\n"
     ]
    }
   ],
   "source": [
    "make_over_folds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load_over_folds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53773\n",
      "6399\n",
      "53936\n",
      "6236\n",
      "53810\n",
      "6362\n",
      "54097\n",
      "6075\n",
      "54172\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "for i in range(folds):\n",
    "    print(len(over_train_folds[i]))\n",
    "    print(len(over_val_folds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.sum(dim=1).mean()\n",
    "    \n",
    "def acc(preds,targs,thresh=0.0):\n",
    "    preds = (preds > thresh).int()\n",
    "    targs = targs.int()\n",
    "    return (preds==targs).float().mean()\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    epsilon = 1e-7\n",
    "    y_pred = torch.sigmoid(y_pred)\n",
    "    y_true = y_true.type(torch.FloatTensor).cuda()\n",
    "    tp = torch.sum(y_true*y_pred)\n",
    "    tn = torch.sum((1-y_true)*(1-y_pred))\n",
    "    fp = torch.sum((1-y_true)*y_pred)\n",
    "    fn = torch.sum((y_true*(1-y_pred)))\n",
    "    \n",
    "    p = tp / (tp + fp + epsilon)\n",
    "    r = tp / (tp + fn + epsilon)\n",
    "    f1 = 2*p*r / (p+r+epsilon)\n",
    "    f1 = torch.where(torch.isnan(f1), torch.zeros_like(f1), f1)\n",
    "    return 2 - torch.mean(f1)\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    y_pred = torch.sigmoid(y_pred)\n",
    "    y_true = y_true.type(torch.FloatTensor).cuda()\n",
    "    score = torch.sum(2.0*(y_pred*y_true))/torch.sum((y_pred+y_true) + 1e-7)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvnetBuilder_custom():\n",
    "    def __init__(self, f, c, is_multi, is_reg, ps=None, xtra_fc=None, xtra_cut=0, \n",
    "                 custom_head=None, pretrained=True):\n",
    "        self.f,self.c,self.is_multi,self.is_reg,self.xtra_cut = f,c,is_multi,is_reg,xtra_cut\n",
    "        if xtra_fc is None: xtra_fc = [512]\n",
    "        if ps is None: ps = [0.25]*len(xtra_fc) + [0.5]\n",
    "        self.ps,self.xtra_fc = ps,xtra_fc\n",
    "\n",
    "        if f in model_meta: cut,self.lr_cut = model_meta[f]\n",
    "        else: cut,self.lr_cut = 0,0\n",
    "        cut-=xtra_cut\n",
    "        layers = cut_model(f(pretrained), cut)\n",
    "        \n",
    "        #replace first convolutional layer by 4->64 while keeping corresponding weights\n",
    "        #and initializing new weights with zeros\n",
    "        #####################################################\n",
    "        w = layers[0].weight\n",
    "        layers[0] = nn.Conv2d(4,64,kernel_size=(7,7),stride=(2,2),padding=(3, 3), bias=False)\n",
    "        layers[0].weight = torch.nn.Parameter(torch.cat((w,torch.zeros(64,1,7,7)),dim=1))\n",
    "        #####################################################\n",
    "        \n",
    "        self.nf = model_features[f] if f in model_features else (num_features(layers)*2)\n",
    "        if not custom_head: layers += [AdaptiveConcatPool2d(), Flatten()]\n",
    "        self.top_model = nn.Sequential(*layers)\n",
    "\n",
    "        n_fc = len(self.xtra_fc)+1\n",
    "        if not isinstance(self.ps, list): self.ps = [self.ps]*n_fc\n",
    "\n",
    "        if custom_head: fc_layers = [custom_head]\n",
    "        else: fc_layers = self.get_fc_layers()\n",
    "        self.n_fc = len(fc_layers)\n",
    "        self.fc_model = to_gpu(nn.Sequential(*fc_layers))\n",
    "        if not custom_head: apply_init(self.fc_model, kaiming_normal)\n",
    "        self.model = to_gpu(nn.Sequential(*(layers+fc_layers)))\n",
    "\n",
    "    @property\n",
    "    def name(self): return f'{self.f.__name__}_{self.xtra_cut}'\n",
    "\n",
    "    def create_fc_layer(self, ni, nf, p, actn=None):\n",
    "        res=[nn.BatchNorm1d(num_features=ni)]\n",
    "        if p: res.append(nn.Dropout(p=p))\n",
    "        res.append(nn.Linear(in_features=ni, out_features=nf))\n",
    "        if actn: res.append(actn)\n",
    "        return res\n",
    "\n",
    "    def get_fc_layers(self):\n",
    "        res=[]\n",
    "        ni=self.nf\n",
    "        for i,nf in enumerate(self.xtra_fc):\n",
    "            res += self.create_fc_layer(ni, nf, p=self.ps[i], actn=nn.ReLU())\n",
    "            ni=nf\n",
    "        final_actn = nn.Sigmoid() if self.is_multi else nn.LogSoftmax()\n",
    "        if self.is_reg: final_actn = None\n",
    "        res += self.create_fc_layer(ni, self.c, p=self.ps[-1], actn=final_actn)\n",
    "        return res\n",
    "\n",
    "    def get_layer_groups(self, do_fc=False):\n",
    "        if do_fc:\n",
    "            return [self.fc_model]\n",
    "        idxs = [self.lr_cut]\n",
    "        c = children(self.top_model)\n",
    "        if len(c)==3: c = children(c[0])+c[1:]\n",
    "        lgs = list(split_by_idxs(c,idxs))\n",
    "        return lgs+[self.fc_model]\n",
    "    \n",
    "class ConvLearner(Learner):\n",
    "    def __init__(self, data, models, precompute=False, **kwargs):\n",
    "        self.precompute = False\n",
    "        super().__init__(data, models, **kwargs)\n",
    "        if hasattr(data, 'is_multi') and not data.is_reg and self.metrics is None:\n",
    "            self.metrics = [accuracy_thresh(0.5)] if self.data.is_multi else [accuracy]\n",
    "        if precompute: self.save_fc1()\n",
    "        self.freeze()\n",
    "        self.precompute = precompute\n",
    "\n",
    "    def _get_crit(self, data):\n",
    "        if not hasattr(data, 'is_multi'): return super()._get_crit(data)\n",
    "\n",
    "        return F.l1_loss if data.is_reg else F.binary_cross_entropy if data.is_multi else F.nll_loss\n",
    "\n",
    "    @classmethod\n",
    "    def pretrained(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n",
    "                   pretrained=True, **kwargs):\n",
    "        models = ConvnetBuilder_custom(f, data.c, data.is_multi, data.is_reg,\n",
    "            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=pretrained)\n",
    "        return cls(data, models, precompute, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def lsuv_learner(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n",
    "                  needed_std=1.0, std_tol=0.1, max_attempts=10, do_orthonorm=False, **kwargs):\n",
    "        models = ConvnetBuilder(f, data.c, data.is_multi, data.is_reg,\n",
    "            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=False)\n",
    "        convlearn=cls(data, models, precompute, **kwargs)\n",
    "        convlearn.lsuv_init()\n",
    "        return convlearn\n",
    "    \n",
    "    @property\n",
    "    def model(self): return self.models.fc_model if self.precompute else self.models.model\n",
    "    \n",
    "    def half(self):\n",
    "        if self.fp16: return\n",
    "        self.fp16 = True\n",
    "        if type(self.model) != FP16: self.models.model = FP16(self.model)\n",
    "        if not isinstance(self.models.fc_model, FP16): self.models.fc_model = FP16(self.models.fc_model)\n",
    "    def float(self):\n",
    "        if not self.fp16: return\n",
    "        self.fp16 = False\n",
    "        if type(self.models.model) == FP16: self.models.model = self.model.module.float()\n",
    "        if type(self.models.fc_model) == FP16: self.models.fc_model = self.models.fc_model.module.float()\n",
    "\n",
    "    @property\n",
    "    def data(self): return self.fc_data if self.precompute else self.data_\n",
    "\n",
    "    def create_empty_bcolz(self, n, name):\n",
    "        return bcolz.carray(np.zeros((0,n), np.float32), chunklen=1, mode='w', rootdir=name)\n",
    "\n",
    "    def set_data(self, data, precompute=False):\n",
    "        super().set_data(data)\n",
    "        if precompute:\n",
    "            self.unfreeze()\n",
    "            self.save_fc1()\n",
    "            self.freeze()\n",
    "            self.precompute = True\n",
    "        else:\n",
    "            self.freeze()\n",
    "\n",
    "    def get_layer_groups(self):\n",
    "        return self.models.get_layer_groups(self.precompute)\n",
    "\n",
    "    def summary(self):\n",
    "        precompute = self.precompute\n",
    "        self.precompute = False\n",
    "        res = super().summary()\n",
    "        self.precompute = precompute\n",
    "        return res\n",
    "\n",
    "    def get_activations(self, force=False):\n",
    "        tmpl = f'_{self.models.name}_{self.data.sz}.bc'\n",
    "        # TODO: Somehow check that directory names haven't changed (e.g. added test set)\n",
    "        names = [os.path.join(self.tmp_path, p+tmpl) for p in ('x_act', 'x_act_val', 'x_act_test')]\n",
    "        if os.path.exists(names[0]) and not force:\n",
    "            self.activations = [bcolz.open(p) for p in names]\n",
    "        else:\n",
    "            self.activations = [self.create_empty_bcolz(self.models.nf,n) for n in names]\n",
    "\n",
    "    def save_fc1(self):\n",
    "        self.get_activations()\n",
    "        act, val_act, test_act = self.activations\n",
    "        m=self.models.top_model\n",
    "        if len(self.activations[0])!=len(self.data.trn_ds):\n",
    "            predict_to_bcolz(m, self.data.fix_dl, act)\n",
    "        if len(self.activations[1])!=len(self.data.val_ds):\n",
    "            predict_to_bcolz(m, self.data.val_dl, val_act)\n",
    "        if self.data.test_dl and (len(self.activations[2])!=len(self.data.test_ds)):\n",
    "            if self.data.test_dl: predict_to_bcolz(m, self.data.test_dl, test_act)\n",
    "\n",
    "        self.fc_data = ImageClassifierData.from_arrays(self.data.path,\n",
    "                (act, self.data.trn_y), (val_act, self.data.val_y), self.data.bs, classes=self.data.classes,\n",
    "                test = test_act if self.data.test_dl else None, num_workers=8)\n",
    "\n",
    "    def freeze(self):\n",
    "        self.freeze_to(-1)\n",
    "\n",
    "    def unfreeze(self):\n",
    "        self.freeze_to(0)\n",
    "        self.precompute = False\n",
    "\n",
    "    def predict_array(self, arr):\n",
    "        precompute = self.precompute\n",
    "        self.precompute = False\n",
    "        pred = super().predict_array(arr)\n",
    "        self.precompute = precompute\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SE Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_settings = {\n",
    "    'pnasnet5large': {\n",
    "        'imagenet': {\n",
    "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/pnasnet5large-bf079911.pth',\n",
    "            'input_space': 'RGB',\n",
    "            'input_size': [3, 331, 331],\n",
    "            'input_range': [0, 1],\n",
    "            'mean': [0.5, 0.5, 0.5],\n",
    "            'std': [0.5, 0.5, 0.5],\n",
    "            'num_classes': 1000\n",
    "        },\n",
    "        'imagenet+background': {\n",
    "            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/pnasnet5large-bf079911.pth',\n",
    "            'input_space': 'RGB',\n",
    "            'input_size': [3, 331, 331],\n",
    "            'input_range': [0, 1],\n",
    "            'mean': [0.5, 0.5, 0.5],\n",
    "            'std': [0.5, 0.5, 0.5],\n",
    "            'num_classes': 1001\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "class MaxPool(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size, stride=1, padding=1, zero_pad=False):\n",
    "        super(MaxPool, self).__init__()\n",
    "        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0)) if zero_pad else None\n",
    "        self.pool = nn.MaxPool2d(kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.zero_pad:\n",
    "            x = self.zero_pad(x)\n",
    "        x = self.pool(x)\n",
    "        if self.zero_pad:\n",
    "            x = x[:, :, 1:, 1:]\n",
    "        return x\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, dw_kernel_size, dw_stride,\n",
    "                 dw_padding):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise_conv2d = nn.Conv2d(in_channels, in_channels,\n",
    "                                          kernel_size=dw_kernel_size,\n",
    "                                          stride=dw_stride, padding=dw_padding,\n",
    "                                          groups=in_channels, bias=False)\n",
    "        self.pointwise_conv2d = nn.Conv2d(in_channels, out_channels,\n",
    "                                          kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise_conv2d(x)\n",
    "        x = self.pointwise_conv2d(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BranchSeparables(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 stem_cell=False, zero_pad=False):\n",
    "        super(BranchSeparables, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        middle_channels = out_channels if stem_cell else in_channels\n",
    "        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0)) if zero_pad else None\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.separable_1 = SeparableConv2d(in_channels, middle_channels,\n",
    "                                           kernel_size, dw_stride=stride,\n",
    "                                           dw_padding=padding)\n",
    "        self.bn_sep_1 = nn.BatchNorm2d(middle_channels, eps=0.001)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.separable_2 = SeparableConv2d(middle_channels, out_channels,\n",
    "                                           kernel_size, dw_stride=1,\n",
    "                                           dw_padding=padding)\n",
    "        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu_1(x)\n",
    "        if self.zero_pad:\n",
    "            x = self.zero_pad(x)\n",
    "        x = self.separable_1(x)\n",
    "        if self.zero_pad:\n",
    "            x = x[:, :, 1:, 1:].contiguous()\n",
    "        x = self.bn_sep_1(x)\n",
    "        x = self.relu_2(x)\n",
    "        x = self.separable_2(x)\n",
    "        x = self.bn_sep_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ReluConvBn(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        super(ReluConvBn, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FactorizedReduction(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FactorizedReduction, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.path_1 = nn.Sequential(OrderedDict([\n",
    "            ('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False)),\n",
    "            ('conv', nn.Conv2d(in_channels, out_channels // 2,\n",
    "                               kernel_size=1, bias=False)),\n",
    "        ]))\n",
    "        self.path_2 = nn.Sequential(OrderedDict([\n",
    "            ('pad', nn.ZeroPad2d((0, 1, 0, 1))),\n",
    "            ('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False)),\n",
    "            ('conv', nn.Conv2d(in_channels, out_channels // 2,\n",
    "                               kernel_size=1, bias=False)),\n",
    "        ]))\n",
    "        self.final_path_bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x_path1 = self.path_1(x)\n",
    "\n",
    "        x_path2 = self.path_2.pad(x)\n",
    "        x_path2 = x_path2[:, :, 1:, 1:]\n",
    "        x_path2 = self.path_2.avgpool(x_path2)\n",
    "        x_path2 = self.path_2.conv(x_path2)\n",
    "\n",
    "        out = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n",
    "        return out\n",
    "\n",
    "\n",
    "class CellBase(nn.Module):\n",
    "\n",
    "    def cell_forward(self, x_left, x_right):\n",
    "        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n",
    "        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n",
    "        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n",
    "\n",
    "        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n",
    "        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n",
    "        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n",
    "\n",
    "        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n",
    "        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n",
    "        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n",
    "\n",
    "        x_comb_iter_3_left = self.comb_iter_3_left(x_comb_iter_2)\n",
    "        x_comb_iter_3_right = self.comb_iter_3_right(x_right)\n",
    "        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n",
    "\n",
    "        x_comb_iter_4_left = self.comb_iter_4_left(x_left)\n",
    "        if self.comb_iter_4_right:\n",
    "            x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n",
    "        else:\n",
    "            x_comb_iter_4_right = x_right\n",
    "        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n",
    "\n",
    "        x_out = torch.cat(\n",
    "            [x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3,\n",
    "             x_comb_iter_4], 1)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class CellStem0(CellBase):\n",
    "\n",
    "    def __init__(self, in_channels_left, out_channels_left, in_channels_right,\n",
    "                 out_channels_right):\n",
    "        super(CellStem0, self).__init__()\n",
    "        self.conv_1x1 = ReluConvBn(in_channels_right, out_channels_right,\n",
    "                                   kernel_size=1)\n",
    "        self.comb_iter_0_left = BranchSeparables(in_channels_left,\n",
    "                                                 out_channels_left,\n",
    "                                                 kernel_size=5, stride=2,\n",
    "                                                 stem_cell=True)\n",
    "        self.comb_iter_0_right = nn.Sequential(OrderedDict([\n",
    "            ('max_pool', MaxPool(3, stride=2)),\n",
    "            ('conv', nn.Conv2d(in_channels_left, out_channels_left,\n",
    "                               kernel_size=1, bias=False)),\n",
    "            ('bn', nn.BatchNorm2d(out_channels_left, eps=0.001)),\n",
    "        ]))\n",
    "        self.comb_iter_1_left = BranchSeparables(out_channels_right,\n",
    "                                                 out_channels_right,\n",
    "                                                 kernel_size=7, stride=2)\n",
    "        self.comb_iter_1_right = MaxPool(3, stride=2)\n",
    "        self.comb_iter_2_left = BranchSeparables(out_channels_right,\n",
    "                                                 out_channels_right,\n",
    "                                                 kernel_size=5, stride=2)\n",
    "        self.comb_iter_2_right = BranchSeparables(out_channels_right,\n",
    "                                                  out_channels_right,\n",
    "                                                  kernel_size=3, stride=2)\n",
    "        self.comb_iter_3_left = BranchSeparables(out_channels_right,\n",
    "                                                 out_channels_right,\n",
    "                                                 kernel_size=3)\n",
    "        self.comb_iter_3_right = MaxPool(3, stride=2)\n",
    "        self.comb_iter_4_left = BranchSeparables(in_channels_right,\n",
    "                                                 out_channels_right,\n",
    "                                                 kernel_size=3, stride=2,\n",
    "                                                 stem_cell=True)\n",
    "        self.comb_iter_4_right = ReluConvBn(out_channels_right,\n",
    "                                            out_channels_right,\n",
    "                                            kernel_size=1, stride=2)\n",
    "\n",
    "    def forward(self, x_left):\n",
    "        x_right = self.conv_1x1(x_left)\n",
    "        x_out = self.cell_forward(x_left, x_right)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class Cell(CellBase):\n",
    "\n",
    "    def __init__(self, in_channels_left, out_channels_left, in_channels_right,\n",
    "                 out_channels_right, is_reduction=False, zero_pad=False,\n",
    "                 match_prev_layer_dimensions=False):\n",
    "        super(Cell, self).__init__()\n",
    "\n",
    "        # If `is_reduction` is set to `True` stride 2 is used for\n",
    "        # convolutional and pooling layers to reduce the spatial size of\n",
    "        # the output of a cell approximately by a factor of 2.\n",
    "        stride = 2 if is_reduction else 1\n",
    "\n",
    "        # If `match_prev_layer_dimensions` is set to `True`\n",
    "        # `FactorizedReduction` is used to reduce the spatial size\n",
    "        # of the left input of a cell approximately by a factor of 2.\n",
    "        self.match_prev_layer_dimensions = match_prev_layer_dimensions\n",
    "        if match_prev_layer_dimensions:\n",
    "            self.conv_prev_1x1 = FactorizedReduction(in_channels_left,\n",
    "                                                     out_channels_left)\n",
    "        else:\n",
    "            self.conv_prev_1x1 = ReluConvBn(in_channels_left,\n",
    "                                            out_channels_left, kernel_size=1)\n",
    "\n",
    "        self.conv_1x1 = ReluConvBn(in_channels_right, out_channels_right,\n",
    "                                   kernel_size=1)\n",
    "        self.comb_iter_0_left = BranchSeparables(out_channels_left,\n",
    "                                                 out_channels_left,\n",
    "                                                 kernel_size=5, stride=stride,\n",
    "                                                 zero_pad=zero_pad)\n",
    "        self.comb_iter_0_right = MaxPool(3, stride=stride, zero_pad=zero_pad)\n",
    "        self.comb_iter_1_left = BranchSeparables(out_channels_right,\n",
    "                                                 out_channels_right,\n",
    "                                                 kernel_size=7, stride=stride,\n",
    "                                                 zero_pad=zero_pad)\n",
    "        self.comb_iter_1_right = MaxPool(3, stride=stride, zero_pad=zero_pad)\n",
    "        self.comb_iter_2_left = BranchSeparables(out_channels_right,\n",
    "                                                 out_channels_right,\n",
    "                                                 kernel_size=5, stride=stride,\n",
    "                                                 zero_pad=zero_pad)\n",
    "        self.comb_iter_2_right = BranchSeparables(out_channels_right,\n",
    "                                                  out_channels_right,\n",
    "                                                  kernel_size=3, stride=stride,\n",
    "                                                  zero_pad=zero_pad)\n",
    "        self.comb_iter_3_left = BranchSeparables(out_channels_right,\n",
    "                                                 out_channels_right,\n",
    "                                                 kernel_size=3)\n",
    "        self.comb_iter_3_right = MaxPool(3, stride=stride, zero_pad=zero_pad)\n",
    "        self.comb_iter_4_left = BranchSeparables(out_channels_left,\n",
    "                                                 out_channels_left,\n",
    "                                                 kernel_size=3, stride=stride,\n",
    "                                                 zero_pad=zero_pad)\n",
    "        if is_reduction:\n",
    "            self.comb_iter_4_right = ReluConvBn(out_channels_right,\n",
    "                                                out_channels_right,\n",
    "                                                kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.comb_iter_4_right = None\n",
    "\n",
    "    def forward(self, x_left, x_right):\n",
    "        x_left = self.conv_prev_1x1(x_left)\n",
    "        x_right = self.conv_1x1(x_right)\n",
    "        x_out = self.cell_forward(x_left, x_right)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class PNASNet5Large(nn.Module):\n",
    "    def __init__(self, num_classes=1001):\n",
    "        super(PNASNet5Large, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.conv_0 = nn.Sequential(OrderedDict([\n",
    "            ('conv', nn.Conv2d(3, 96, kernel_size=3, stride=2, bias=False)),\n",
    "            ('bn', nn.BatchNorm2d(96, eps=0.001))\n",
    "        ]))\n",
    "        self.cell_stem_0 = CellStem0(in_channels_left=96, out_channels_left=54,\n",
    "                                     in_channels_right=96,\n",
    "                                     out_channels_right=54)\n",
    "        self.cell_stem_1 = Cell(in_channels_left=96, out_channels_left=108,\n",
    "                                in_channels_right=270, out_channels_right=108,\n",
    "                                match_prev_layer_dimensions=True,\n",
    "                                is_reduction=True)\n",
    "        self.cell_0 = Cell(in_channels_left=270, out_channels_left=216,\n",
    "                           in_channels_right=540, out_channels_right=216,\n",
    "                           match_prev_layer_dimensions=True)\n",
    "        self.cell_1 = Cell(in_channels_left=540, out_channels_left=216,\n",
    "                           in_channels_right=1080, out_channels_right=216)\n",
    "        self.cell_2 = Cell(in_channels_left=1080, out_channels_left=216,\n",
    "                           in_channels_right=1080, out_channels_right=216)\n",
    "        self.cell_3 = Cell(in_channels_left=1080, out_channels_left=216,\n",
    "                           in_channels_right=1080, out_channels_right=216)\n",
    "        self.cell_4 = Cell(in_channels_left=1080, out_channels_left=432,\n",
    "                           in_channels_right=1080, out_channels_right=432,\n",
    "                           is_reduction=True, zero_pad=True)\n",
    "        self.cell_5 = Cell(in_channels_left=1080, out_channels_left=432,\n",
    "                           in_channels_right=2160, out_channels_right=432,\n",
    "                           match_prev_layer_dimensions=True)\n",
    "        self.cell_6 = Cell(in_channels_left=2160, out_channels_left=432,\n",
    "                           in_channels_right=2160, out_channels_right=432)\n",
    "        self.cell_7 = Cell(in_channels_left=2160, out_channels_left=432,\n",
    "                           in_channels_right=2160, out_channels_right=432)\n",
    "        self.cell_8 = Cell(in_channels_left=2160, out_channels_left=864,\n",
    "                           in_channels_right=2160, out_channels_right=864,\n",
    "                           is_reduction=True)\n",
    "        self.cell_9 = Cell(in_channels_left=2160, out_channels_left=864,\n",
    "                           in_channels_right=4320, out_channels_right=864,\n",
    "                           match_prev_layer_dimensions=True)\n",
    "        self.cell_10 = Cell(in_channels_left=4320, out_channels_left=864,\n",
    "                            in_channels_right=4320, out_channels_right=864)\n",
    "        self.cell_11 = Cell(in_channels_left=4320, out_channels_left=864,\n",
    "                            in_channels_right=4320, out_channels_right=864)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.avg_pool = nn.AvgPool2d(11, stride=1, padding=0)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.last_linear = nn.Linear(4320, num_classes)\n",
    "\n",
    "    def features(self, x):\n",
    "        x_conv_0 = self.conv_0(x)\n",
    "        x_stem_0 = self.cell_stem_0(x_conv_0)\n",
    "        x_stem_1 = self.cell_stem_1(x_conv_0, x_stem_0)\n",
    "        x_cell_0 = self.cell_0(x_stem_0, x_stem_1)\n",
    "        x_cell_1 = self.cell_1(x_stem_1, x_cell_0)\n",
    "        x_cell_2 = self.cell_2(x_cell_0, x_cell_1)\n",
    "        x_cell_3 = self.cell_3(x_cell_1, x_cell_2)\n",
    "        x_cell_4 = self.cell_4(x_cell_2, x_cell_3)\n",
    "        x_cell_5 = self.cell_5(x_cell_3, x_cell_4)\n",
    "        x_cell_6 = self.cell_6(x_cell_4, x_cell_5)\n",
    "        x_cell_7 = self.cell_7(x_cell_5, x_cell_6)\n",
    "        x_cell_8 = self.cell_8(x_cell_6, x_cell_7)\n",
    "        x_cell_9 = self.cell_9(x_cell_7, x_cell_8)\n",
    "        x_cell_10 = self.cell_10(x_cell_8, x_cell_9)\n",
    "        x_cell_11 = self.cell_11(x_cell_9, x_cell_10)\n",
    "        return x_cell_11\n",
    "\n",
    "    def logits(self, features):\n",
    "        x = self.relu(features)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.features(input)\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNASConvBuilder(ConvnetBuilder_custom):\n",
    "    def __init__(self, f, c, is_multi, is_reg, ps=None, xtra_fc=None, xtra_cut=0, \n",
    "                 custom_head=None, pretrained=True):\n",
    "        self.f,self.c,self.is_multi,self.is_reg,self.xtra_cut = f,c,is_multi,is_reg,xtra_cut\n",
    "        if xtra_fc is None: xtra_fc = [512]\n",
    "        if ps is None: ps = [0.25]*len(xtra_fc) + [0.5]\n",
    "        self.ps,self.xtra_fc = ps,xtra_fc\n",
    "\n",
    "        if f in model_meta: cut,self.lr_cut = model_meta[f]\n",
    "        else: cut,self.lr_cut = 8,6\n",
    "        cut-=xtra_cut\n",
    "        layers = cut_model(f(), 5)\n",
    "        \n",
    "        #replace first convolutional layer by 4->64 while keeping corresponding weights\n",
    "        #and initializing new weights with zeros\n",
    "        #####################################################\n",
    "        w = layers[0].conv1.weight\n",
    "        w1 = layers[0].conv1.weight[:,0].unsqueeze(dim=1)\n",
    "        layers[0].conv1 = nn.Conv2d(4,64,kernel_size=(7,7),stride=(2,2),padding=(3, 3), bias=False)\n",
    "        layers[0].conv1.weight = torch.nn.Parameter(torch.cat((w,w1),dim=1))\n",
    "        #####################################################\n",
    "        \n",
    "        self.nf = model_features[f] if f in model_features else (num_features(layers)*2)\n",
    "        if not custom_head: layers += [AdaptiveConcatPool2d(), Flatten()]\n",
    "        self.top_model = nn.Sequential(*layers)\n",
    "\n",
    "        n_fc = len(self.xtra_fc)+1\n",
    "        if not isinstance(self.ps, list): self.ps = [self.ps]*n_fc\n",
    "\n",
    "        if custom_head: fc_layers = [custom_head]\n",
    "        else: fc_layers = self.get_fc_layers()\n",
    "        self.n_fc = len(fc_layers)\n",
    "        self.fc_model = to_gpu(nn.Sequential(*fc_layers))\n",
    "        if not custom_head: apply_init(self.fc_model, kaiming_normal)\n",
    "        self.model = to_gpu(nn.Sequential(*(layers+fc_layers)))\n",
    "\n",
    "class PNASNet(ConvLearner):\n",
    "        @classmethod\n",
    "        def pretrained(cls, f, data, ps=None, xtra_fc=None, xtra_cut=0, custom_head=None, precompute=False,\n",
    "                   pretrained=True, **kwargs):\n",
    "            models = PNASConvBuilder(f, data.c, data.is_multi, data.is_reg,\n",
    "            ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=pretrained)\n",
    "            return cls(data, models, precompute, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnasnet5large(num_classes=1001, pretrained='imagenet+background'):\n",
    "    r\"\"\"PNASNet-5 model architecture from the\n",
    "    `\"Progressive Neural Architecture Search\"\n",
    "    <https://arxiv.org/abs/1712.00559>`_ paper.\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        settings = pretrained_settings['pnasnet5large'][pretrained]\n",
    "        assert num_classes == settings[\n",
    "            'num_classes'], 'num_classes should be {}, but is {}'.format(\n",
    "            settings['num_classes'], num_classes)\n",
    "\n",
    "        # both 'imagenet'&'imagenet+background' are loaded from same parameters\n",
    "        model = PNASNet5Large(num_classes=1001)\n",
    "        model.load_state_dict(model_zoo.load_url(settings['url']))\n",
    "\n",
    "        if pretrained == 'imagenet':\n",
    "            new_last_linear = nn.Linear(model.last_linear.in_features, 1000)\n",
    "            new_last_linear.weight.data = model.last_linear.weight.data[1:]\n",
    "            new_last_linear.bias.data = model.last_linear.bias.data[1:]\n",
    "            model.last_linear = new_last_linear\n",
    "\n",
    "        model.input_space = settings['input_space']\n",
    "        model.input_size = settings['input_size']\n",
    "        model.input_range = settings['input_range']\n",
    "\n",
    "        model.mean = settings['mean']\n",
    "        model.std = settings['std']\n",
    "    else:\n",
    "        model = PNASNet5Large(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold(sz, bs, k):\n",
    "    aug_tfms = [RandomRotate(30, tfm_y=TfmType.NO),\n",
    "                RandomDihedral(tfm_y=TfmType.NO),\n",
    "                RandomLighting(0.05, 0.05, tfm_y=TfmType.NO)]\n",
    "    \n",
    "    stats = A([0.00505, 0.00331, 0.00344, 0.00519], [0.10038, 0.08131, 0.08284, 0.10179])\n",
    "    tfms = tfms_from_stats(stats, sz, crop_type=CropType.NO, tfm_y=TfmType.NO, \n",
    "                aug_tfms=aug_tfms)\n",
    "    \n",
    "    trn_x = list(train_folds[k]['Id'])\n",
    "    val_x = list(val_folds[k]['Id'])\n",
    "    \n",
    "    if len(trn_x)%bs == 0:\n",
    "        ds = ImageData.get_ds(pdFilesDataset, (trn_x,TRAIN), \n",
    "                (val_x,TRAIN), tfms, test=(test_names,TEST))\n",
    "    else:\n",
    "        ds = ImageData.get_ds(pdFilesDataset, (trn_x[:-(len(trn_x)%bs)],TRAIN), \n",
    "                (val_x,TRAIN), tfms, test=(test_names,TEST))\n",
    "    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pnas_model(md):\n",
    "    learn = PNASNet.pretrained(pnasnet5large, md, ps=0.5) #dropout 50%\n",
    "    learn.opt_fn = optim.Adam\n",
    "    learn.crit = FocalLoss()\n",
    "    learn.metrics = [acc, f1_metric]\n",
    "    learn.clip = 1.0\n",
    "    return learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_model(sz, bs, k):\n",
    "    md = get_fold(sz,bs,k)\n",
    "    learn = get_pnas_model(md)\n",
    "    learn.freeze_to(1)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"http://data.lip6.fr/cadene/pretrainedmodels/pnasnet5large-bf079911.pth\" to /home/eigenstir/.torch/models/pnasnet5large-bf079911.pth\n",
      "100%|██████████| 345153926/345153926 [09:12<00:00, 624269.72it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'conv1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-01054dbd8a20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fold_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-eea3a5039587>\u001b[0m in \u001b[0;36mget_fold_model\u001b[0;34m(sz, bs, k)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_fold_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pnas_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-04b66f78b0d9>\u001b[0m in \u001b[0;36mget_pnas_model\u001b[0;34m(md)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_pnas_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPNASNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpnasnet5large\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#dropout 50%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFocalLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_metric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-bb11ba69ffa0>\u001b[0m in \u001b[0;36mpretrained\u001b[0;34m(cls, f, data, ps, xtra_fc, xtra_cut, custom_head, precompute, pretrained, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m                    pretrained=True, **kwargs):\n\u001b[1;32m     41\u001b[0m             models = PNASConvBuilder(f, data.c, data.is_multi, data.is_reg,\n\u001b[0;32m---> 42\u001b[0;31m             ps=ps, xtra_fc=xtra_fc, xtra_cut=xtra_cut, custom_head=custom_head, pretrained=pretrained)\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-bb11ba69ffa0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, c, is_multi, is_reg, ps, xtra_fc, xtra_cut, custom_head, pretrained)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#and initializing new weights with zeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 518\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'conv1'"
     ]
    }
   ],
   "source": [
    "sz = 256\n",
    "bs = 16\n",
    "nw=6\n",
    "learn = get_fold_model(sz,bs,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Oversample Edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_over_fold(sz, bs, k):\n",
    "    aug_tfms = [RandomRotate(30, tfm_y=TfmType.NO),\n",
    "                RandomDihedral(tfm_y=TfmType.NO),\n",
    "                RandomLighting(0.05, 0.05, tfm_y=TfmType.NO)]\n",
    "    \n",
    "    stats = A([0.00505, 0.00331, 0.00344, 0.00519], [0.10038, 0.08131, 0.08284, 0.10179])\n",
    "    tfms = tfms_from_stats(stats, sz, crop_type=CropType.NO, tfm_y=TfmType.NO, \n",
    "                aug_tfms=aug_tfms)\n",
    "    \n",
    "    trn_x = list(over_train_folds[k]['Id'])\n",
    "    val_x = list(over_val_folds[k]['Id'])\n",
    "    \n",
    "    if len(trn_x)%bs == 0:\n",
    "        ds = ImageData.get_ds(pdFilesDataset, (trn_x,TRAIN), \n",
    "                (val_x,TRAIN), tfms, test=(test_names,TEST))\n",
    "    else:\n",
    "        ds = ImageData.get_ds(pdFilesDataset, (trn_x[:-(len(trn_x)%bs)],TRAIN), \n",
    "                (val_x,TRAIN), tfms, test=(test_names,TEST))\n",
    "    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n",
    "    return md\n",
    "\n",
    "def get_overfold_model(sz, bs, k):\n",
    "    md = get_over_fold(sz,bs,k)\n",
    "    learn = get_resnext50_model(md)\n",
    "    learn.freeze_to(1)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Overfold Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "wd = 1e-7\n",
    "nw=6\n",
    "lrs=np.array([lr/10,lr/3,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sz = 128\n",
    "bs = 24\n",
    "for k in range(folds):\n",
    "    learn = get_overfold_model(sz,bs,k)\n",
    "    learn.fit(lr, 1, wds=wd, cycle_len=8, use_clr_beta=(5,8,0.85,0.9), use_wd_sched=True)\n",
    "    \n",
    "    learn.unfreeze()\n",
    "    \n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=10, use_clr_beta=(20,10,0.85,0.9), use_wd_sched=True)\n",
    "    learn.save(f'SEResNextFold_128_{k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n",
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b012bb589c4a678bc0b82b5e09285c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   acc        f1_metric         \n",
      "    0      1.287492   1.480285   0.933829   1.411398  \n",
      "    1      1.190252   1.432211   0.936756   1.385831          \n",
      "    2      1.07842    1.427014   0.938588   1.354518          \n",
      "    3      1.050972   1.409981   0.939156   1.326916          \n",
      "    4      0.941893   1.369003   0.942046   1.312287           \n",
      "    5      0.877585   1.367926   0.942392   1.289273           \n",
      "    6      0.863571   1.329484   0.944335   1.286037           \n",
      "    7      0.830794   1.270952   0.94522    1.283957           \n",
      "    8      0.795969   1.318031   0.945266   1.267977           \n",
      "    9      0.800551   1.284401   0.945784   1.275932           \n"
     ]
    }
   ],
   "source": [
    "sz = 256\n",
    "bs = 16\n",
    "for k in range(folds):\n",
    "    learn = get_overfold_model(sz,bs,k)\n",
    "    learn.load(f'SEResNextFold_128_{k}')\n",
    "    learn.fit(lr, 1, wds=wd, cycle_len=8, use_clr_beta=(5,5,0.85,0.9), use_wd_sched=True)\n",
    "    \n",
    "    learn.unfreeze()\n",
    "    learn.bn_freeze(True)\n",
    "    \n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=20, use_clr_beta=(20,8,0.85,0.9), use_wd_sched=True)\n",
    "    learn.save(f'SEResNextFold_256_{k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n",
      "/home/eigenstir/anaconda3/lib/python3.6/site-packages/fastai/initializers.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  if hasattr(m, 'weight'): init_fn(m.weight)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae25459c68847b9b190579b187de4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   acc        f1_metric          \n",
      "    0      0.936123   1.493907   0.939811   1.257735  \n",
      "    1      0.909803   1.498809   0.942874   1.244564           \n",
      "    2      0.909558   1.519219   0.943788   1.222911           \n",
      "    3      0.874937   1.512659   0.94534    1.212539           \n",
      "    4      0.770423   1.367456   0.948469   1.219535           \n",
      "    5      0.801867   1.5168     0.947616   1.199073           \n",
      "    6      0.761069   1.317298   0.951853   1.197406           \n",
      "    7      0.714581   1.306861   0.951408   1.199208           \n",
      "    8      0.699356   1.419535   0.953409   1.186518           \n",
      "    9      0.662387   1.282365   0.955352   1.191486           \n",
      "    10     0.656751   1.251562   0.953623   1.178844           \n",
      "    11     0.659572   1.137671   0.956575   1.189399           \n",
      "    12     0.6996     1.205377   0.957271   1.179339           \n",
      "    13     0.662278   1.083555   0.958654   1.178905           \n",
      "    14     0.626937   1.007507   0.959074   1.192847           \n",
      "    15     0.644517   1.043043   0.959152   1.186507           \n",
      "    16     0.58247    1.004871   0.959284   1.193306           \n",
      " 41%|████      | 1767/4324 [17:47<25:44,  1.66it/s, loss=0.621]"
     ]
    }
   ],
   "source": [
    "sz = 512\n",
    "bs = 8\n",
    "for k in range(folds):\n",
    "    learn = get_overfold_model(sz,bs,3)\n",
    "    learn.load(f'SEResNextFold_256_{k}')\n",
    "    learn.fit(lr, 1, wds=wd, cycle_len=8, use_clr_beta=(5,5,0.85,0.9), use_wd_sched=True)\n",
    "    \n",
    "    learn.unfreeze()\n",
    "    learn.bn_freeze(True)\n",
    "    \n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=20, use_clr_beta=(20,8,0.85,0.9), use_wd_sched=True, best_save_name='SEResNextFold_512_3best')\n",
    "    learn.save(f'SEResNextFold_512_{k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred = make_prediction()\n",
    "check_thresh(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_pred(pred, th_t, fname=f'protein_class_3_oversample_tht.csv')\n",
    "save_pred(pred, 0.5, fname=f'protein_class_3_oversample_0.5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_thresh(preds_y):\n",
    "    thresholds = np.linspace(0, 1, 1500)\n",
    "    score = 0.0\n",
    "    best_threshold=0.0\n",
    "    best_val = 0.0\n",
    "    for threshold in thresholds:\n",
    "        score = f1_score(valid_y > 0.5, preds_y > threshold, average='macro')\n",
    "        if score > best_val:\n",
    "            best_threshold = threshold\n",
    "            best_val = score\n",
    "        print(\"Threshold %0.4f, F1: %0.4f\" % (threshold,score))\n",
    "\n",
    "    print(\"BEST: %0.5f, F1: %0.5f\" % (best_threshold,best_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_t = np.array([0.565,0.39,0.55,0.345,0.33,0.39,0.33,0.45,0.38,0.39,\n",
    "               0.34,0.42,0.31,0.38,0.49,0.50,0.38,0.43,0.46,0.40,\n",
    "               0.39,0.505,0.37,0.47,0.41,0.545,0.32,0.1])\n",
    "\n",
    "def make_prediction():\n",
    "    preds,y = learn.TTA(n_aug=8, is_test=True)\n",
    "    preds = np.stack(preds, axis=-1)\n",
    "    preds = sigmoid_np(preds)\n",
    "    pred = preds.max(axis=-1)\n",
    "    return pred\n",
    "\n",
    "def save_pred(pred, th=0.5, fname='protein_classification.csv'):\n",
    "    pred_list = []\n",
    "    for line in pred:\n",
    "        s = ' '.join(list([str(i) for i in np.nonzero(line>th)[0]]))\n",
    "        pred_list.append(s)\n",
    "        \n",
    "    sample_df = pd.read_csv(SAMPLE)\n",
    "    sample_list = list(sample_df.Id)\n",
    "    pred_dic = dict((key, value) for (key, value) \n",
    "                in zip(learn.data.test_ds.fnames,pred_list))\n",
    "    pred_list_cor = [pred_dic[id] for id in sample_list]\n",
    "    df = pd.DataFrame({'Id':sample_list,'Predicted':pred_list_cor})\n",
    "    df.to_csv(fname, header=True, index=False)\n",
    "\n",
    "def sigmoid_np(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
